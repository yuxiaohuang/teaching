{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGFG86G217fM",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Models\" data-toc-modified-id=\"Models-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression\" data-toc-modified-id=\"Linear-regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Linear regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-normal-equation\" data-toc-modified-id=\"The-normal-equation-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The normal equation</a></span></li><li><span><a href=\"#Batch-gradient-descent-(BGD)\" data-toc-modified-id=\"Batch-gradient-descent-(BGD)-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Batch gradient descent (BGD)</a></span></li><li><span><a href=\"#Stochastic-gradient-descent-(SGD)\" data-toc-modified-id=\"Stochastic-gradient-descent-(SGD)-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Stochastic gradient descent (SGD)</a></span></li><li><span><a href=\"#Mini-batch-gradient-descent-(MBGD)\" data-toc-modified-id=\"Mini-batch-gradient-descent-(MBGD)-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Mini-batch gradient descent (MBGD)</a></span></li></ul></li><li><span><a href=\"#Logistic-regression\" data-toc-modified-id=\"Logistic-regression-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Logistic regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mini-batch-gradient-descent-(MBGD)\" data-toc-modified-id=\"Mini-batch-gradient-descent-(MBGD)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Mini-batch gradient descent (MBGD)</a></span></li></ul></li><li><span><a href=\"#Shallow-neural-networks\" data-toc-modified-id=\"Shallow-neural-networks-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Shallow neural networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Single-layer-perceptron\" data-toc-modified-id=\"Single-layer-perceptron-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Single-layer perceptron</a></span></li><li><span><a href=\"#Multiple-layer-perceptron\" data-toc-modified-id=\"Multiple-layer-perceptron-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Multiple-layer perceptron</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0iSrdHP17fP"
   },
   "source": [
    "<b>\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "Popular Machine Learning Methods: Idea, Practice and Math\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Models: Shallow Learning\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Yuxiao Huang\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "froUMaTq17fQ"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs8b9gZt17fS"
   },
   "source": [
    "- This notebook includes some common models used in PMLM.\n",
    "- Concretely, these models are:\n",
    "    - linear regression\n",
    "    - logistic regression\n",
    "    - single / multiple layer perceptron \n",
    "- See the accompanied slides in our [github repository](https://github.com/yuxiaohuang/teaching/tree/master/gwu/machine_learning_I/fall_2020/slides)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuNO5H9y17fU"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXOL0bd017fW"
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GfS9Suw17fX"
   },
   "source": [
    "### The normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQaNdBpD17fZ"
   },
   "source": [
    "The code below shows how to implement linear regression using the normal equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1601408797799,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "rRQpBrAI17fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_NE(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using the normal equation\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        y : the target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix, [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # Get the optimal solution using the normal equation\n",
    "        self.theta = np.linalg.pinv(IX).dot(y)\n",
    "        \n",
    "        # Get the predicted target vector\n",
    "        y_pred = self.net_input(IX)\n",
    "                        \n",
    "        # Get the loss (MSE)\n",
    "        self.loss = ((y - y_pred) ** 2).sum() / IX.shape[0]\n",
    "        \n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHZAarvd17fk"
   },
   "source": [
    "### Batch gradient descent (BGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mH7feza17fm"
   },
   "source": [
    "The code below shows how to implement linear regression using batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 645,
     "status": "ok",
     "timestamp": 1601408797800,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "qYsTNToH17fn"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_BGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_iter=100, \n",
    "                 eta=10 ** -2,\n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "\n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the predicted target vector on the training data\n",
    "            y_train_pred = self.net_input(IX_train)\n",
    "            \n",
    "            # Get the training error\n",
    "            error_train = y_train - y_train_pred\n",
    "                        \n",
    "            # Get the training mse\n",
    "            mse_train = (error_train ** 2).sum() / IX_train.shape[0]\n",
    "            \n",
    "            # Update the parameters\n",
    "            # If no regularization\n",
    "            if self.penalty == None:\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train))\n",
    "            # If lasso\n",
    "            elif self.penalty == 'l1':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "            # If ridge\n",
    "            elif self.penalty == 'l2':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * np.append([0], self.theta[1:]))\n",
    "            # If elastic net\n",
    "            elif self.penalty == 'elasticnet':\n",
    "                self.theta += self.eta * (2 / IX_train.shape[0] * IX_train.T.dot(error_train) \n",
    "                                          - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                          - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "                           \n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "                \n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zufbpy-S17fs"
   },
   "source": [
    "### Stochastic gradient descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iwyaosu17fs"
   },
   "source": [
    "The code below shows how to implement linear regression using stochastic gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 795,
     "status": "ok",
     "timestamp": 1601408797952,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "CPNHUh7S17fu"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_SGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using stochastic gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the indices of the augmented training feature matrix\n",
    "        idxs_train = np.array(range(IX_train.shape[0]))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            if self.shuffle is True:\n",
    "                # Shuffle the indices\n",
    "                self.rgen.shuffle(idxs_train)\n",
    "                \n",
    "            # Initialize the mse\n",
    "            mse_train = 0\n",
    "            \n",
    "            # For each sample\n",
    "            for i in idxs_train:                \n",
    "                # Get the predicted target vector on the training data\n",
    "                y_train_pred = self.net_input(IX_train[i, :])\n",
    "\n",
    "                # Get the training error\n",
    "                error_train = y_train[i] - y_train_pred\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2) / IX_train.shape[0]\n",
    "\n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], self.theta[1:]))\n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    self.theta += self.eta * (2 * IX_train[i, :].T.dot(error_train) \n",
    "                                              - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                              - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "\n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "\n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "\n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "\n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "\n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "\n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "                \n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alEChMIu17fy"
   },
   "source": [
    "### Mini-batch gradient descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U0dTDvc17fy"
   },
   "source": [
    "The code below shows how to implement linear regression using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1601408798149,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "2ZljWGWn17fz",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegression_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Linear regression implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 batch_size=32,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=0.0001, \n",
    "                 gamma=0.15,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs_train = np.array(range(IX_train.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_minibatches(idxs_train)\n",
    "\n",
    "            # Initialize the training mse\n",
    "            mse_train = 0\n",
    "\n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the augmented training feature matrix and target vector\n",
    "                IX_train_mb, y_train_mb = IX_train[mb,:], y_train[mb]\n",
    "\n",
    "                # Get the predicted target vector on the training data\n",
    "                y_train_mb_pred = self.net_input(IX_train_mb)\n",
    "\n",
    "                # Get the training error\n",
    "                error_train = y_train_mb - y_train_mb_pred\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2).sum() / IX_train.shape[0]\n",
    "                \n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * np.append([0], self.theta[1:]))\n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    self.theta += self.eta * (2 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                              - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                              - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "\n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "                \n",
    "                # Get the predicted target vector on the validation data\n",
    "                y_val_pred = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the validation error\n",
    "                error_val = y_val - y_val_pred\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def get_minibatches(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the predicted target vector\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted value of the target\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        return self.net_input(IX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVP7unFGBRlz"
   },
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqph1EErBRl3"
   },
   "source": [
    "### Mini-batch gradient descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLrJ4jJJBRl3"
   },
   "source": [
    "The code below shows how to implement logistic regression using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1376,
     "status": "ok",
     "timestamp": 1601408798539,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "OIA_IwdiBRl4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Logistic regression implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 batch_size=32,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=1, \n",
    "                 gamma=0.5,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Get the unique classes of the target\n",
    "        self.classes = np.unique(y_train)\n",
    "        \n",
    "        # Get the number of unique classes of the target\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            # Make a copy of y_train\n",
    "            Y_train = np.copy(y_train)       \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the one-hot-encoded training target matrix\n",
    "            Y_train = pd.get_dummies(y_train).values\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=(IX_train.shape[1], self.n_classes))            \n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs_train = np.array(range(IX_train.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_minibatches(idxs_train)\n",
    "\n",
    "            # Initialize the training mse\n",
    "            mse_train = 0\n",
    "\n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the augmented training feature matrix and target matrix\n",
    "                # If binary classification\n",
    "                if self.n_classes == 2:\n",
    "                    IX_train_mb, Y_train_mb = IX_train[mb,:], Y_train[mb]       \n",
    "                # If multi-class classification\n",
    "                else:\n",
    "                    IX_train_mb, Y_train_mb = IX_train[mb,:], Y_train[mb,:]       \n",
    "                                          \n",
    "                # Get the net input matrix\n",
    "                N_train_mb = self.net_input(IX_train_mb)\n",
    "                \n",
    "                # Get the probability matrix\n",
    "                P_train_mb = self.activation(N_train_mb)\n",
    "                                          \n",
    "                # Get the training error\n",
    "                error_train = Y_train_mb - P_train_mb\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2).sum() / IX_train.shape[0]\n",
    "                \n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0))                        \n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))                        \n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                                  - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * self.gamma * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0) \n",
    "                                                  - self.alpha * (1 - self.gamma) * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))\n",
    "                                                \n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "\n",
    "                # If binary classification\n",
    "                if self.n_classes == 2:\n",
    "                    # Make a copy of y_val\n",
    "                    Y_val = np.copy(y_val)            \n",
    "                # If multi-class classification\n",
    "                else:\n",
    "                    # Get the one-hot-encoded validation target matrix\n",
    "                    Y_val = pd.get_dummies(y_val).values\n",
    "                \n",
    "                # Get the net input matrix on the validation data\n",
    "                N_val = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the probability matrix on the validation data\n",
    "                P_val = self.activation(N_val)\n",
    "                                          \n",
    "                # Get the validation error\n",
    "                error_val = Y_val - P_val\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def get_minibatches(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the net input matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The net input matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "    \n",
    "    def activation(self, net_input):\n",
    "        \"\"\"\n",
    "        Get the probability (sigmoid or softmax) matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_input : The net input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            # Get the exponent of the negative net input\n",
    "            neg_net_input_exp = np.exp(-np.clip(net_input, -250, 250))\n",
    "            \n",
    "            # Return the sigmoid matrix\n",
    "            return 1. / (1. + neg_net_input_exp)           \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the exponent of the net input\n",
    "            net_input_exp = np.exp(net_input - np.max(net_input, axis=1).reshape(-1, 1))\n",
    "\n",
    "            # Return the softmax matrix\n",
    "            return net_input_exp / np.sum(net_input_exp, axis=1).reshape(-1, 1)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        The predict probability function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "            \n",
    "        # Get the net_input matrix\n",
    "        N = self.net_input(IX)\n",
    "\n",
    "        return self.activation(N)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict class function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted class vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            return (self.predict_proba(X) >= 0.5) * 1         \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVP7unFGBRlz"
   },
   "source": [
    "## Shallow neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqph1EErBRl3"
   },
   "source": [
    "### Single-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLrJ4jJJBRl3"
   },
   "source": [
    "The code below shows how to implement single-layer perceptron using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1376,
     "status": "ok",
     "timestamp": 1601408798539,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "OIA_IwdiBRl4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class SingleLayerPerceptron_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Single-layer perceptron implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_iter=100,\n",
    "                 shuffle=True,\n",
    "                 batch_size=32,\n",
    "                 eta=10 ** -2, \n",
    "                 penalty='l2',\n",
    "                 alpha=1, \n",
    "                 gamma=0.5,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The learning rate\n",
    "        self.eta = eta\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : The training feature matrix\n",
    "        y_train : The training target vector\n",
    "        X_val : The validation feature matrix\n",
    "        y_val : The validation target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented training feature matrix, [1, X_train]\n",
    "        IX_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n",
    "        \n",
    "        # Get the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Get the unique classes of the target\n",
    "        self.classes = np.unique(y_train)\n",
    "        \n",
    "        # Get the number of unique classes of the target\n",
    "        self.n_classes = len(self.classes)\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            # Make a copy of y_train\n",
    "            Y_train = np.copy(y_train)       \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the one-hot-encoded training target matrix\n",
    "            Y_train = pd.get_dummies(y_train).values\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=IX_train.shape[1])\n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            self.theta = self.rgen.normal(loc=0.0, scale=0.01, size=(IX_train.shape[1], self.n_classes))            \n",
    "        \n",
    "        # Initialize the training and validation loss\n",
    "        self.loss_train, self.loss_val = [], []\n",
    "        \n",
    "        # For each epoch\n",
    "        for _ in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs_train = np.array(range(IX_train.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_minibatches(idxs_train)\n",
    "\n",
    "            # Initialize the training mse\n",
    "            mse_train = 0\n",
    "\n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the augmented training feature matrix and target matrix\n",
    "                # If binary classification\n",
    "                if self.n_classes == 2:\n",
    "                    IX_train_mb, Y_train_mb = IX_train[mb,:], Y_train[mb]       \n",
    "                # If multi-class classification\n",
    "                else:\n",
    "                    IX_train_mb, Y_train_mb = IX_train[mb,:], Y_train[mb,:]       \n",
    "                                          \n",
    "                # Get the net input matrix\n",
    "                N_train_mb = self.net_input(IX_train_mb)\n",
    "                \n",
    "                # Get the output matrix\n",
    "                A_train_mb = self.activation(N_train_mb)\n",
    "                                          \n",
    "                # Get the training error\n",
    "                error_train = Y_train_mb - A_train_mb\n",
    "\n",
    "                # Get the training mse\n",
    "                mse_train += (error_train ** 2).sum() / IX_train.shape[0]\n",
    "                \n",
    "                # Update the parameters\n",
    "                # If no regularization\n",
    "                if self.penalty == None:\n",
    "                    self.theta += self.eta / IX_train_mb.shape[0] * (IX_train_mb.T.dot(error_train))\n",
    "                # If lasso\n",
    "                elif self.penalty == 'l1':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append([0], np.sign(self.theta[1:])))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0))                        \n",
    "                # If ridge\n",
    "                elif self.penalty == 'l2':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))                        \n",
    "                # If elastic net\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    # If binary classification\n",
    "                    if self.n_classes == 2:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * self.gamma * np.append([0], np.sign(self.theta[1:])) \n",
    "                                                  - self.alpha * (1 - self.gamma) * np.append([0], self.theta[1:]))\n",
    "                    # If multi-class classification\n",
    "                    else:\n",
    "                        self.theta += self.eta * (1 / IX_train_mb.shape[0] * IX_train_mb.T.dot(error_train) \n",
    "                                                  - self.alpha * self.gamma * np.append(np.zeros((1, self.theta.shape[1])), np.sign(self.theta[1:,:]), axis=0) \n",
    "                                                  - self.alpha * (1 - self.gamma) * np.append(np.zeros((1, self.theta.shape[1])), self.theta[1:,:], axis=0))\n",
    "                                                \n",
    "            # Update the training loss\n",
    "            self.loss_train.append(mse_train)\n",
    "            \n",
    "            # If the validation feature matrix and target vector are available\n",
    "            if X_val is not None and y_val is not None:\n",
    "                # Get the augmented validation feature matrix, [1, X_val]\n",
    "                IX_val = np.hstack((np.ones((X_val.shape[0], 1)), X_val))\n",
    "\n",
    "                # If binary classification\n",
    "                if self.n_classes == 2:\n",
    "                    # Make a copy of y_val\n",
    "                    Y_val = np.copy(y_val)            \n",
    "                # If multi-class classification\n",
    "                else:\n",
    "                    # Get the one-hot-encoded validation target matrix\n",
    "                    Y_val = pd.get_dummies(y_val).values\n",
    "                \n",
    "                # Get the net input matrix on the validation data\n",
    "                N_val = self.net_input(IX_val)\n",
    "                \n",
    "                # Get the output matrix on the validation data\n",
    "                A_val = self.activation(N_val)\n",
    "                                          \n",
    "                # Get the validation error\n",
    "                error_val = Y_val - A_val\n",
    "                \n",
    "                # Get the validation mse\n",
    "                mse_val = (error_val ** 2).sum() / IX_val.shape[0]\n",
    "                \n",
    "                # Update the validation loss\n",
    "                self.loss_val.append(mse_val)\n",
    "\n",
    "    def get_minibatches(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "\n",
    "    def net_input(self, IX):\n",
    "        \"\"\"\n",
    "        Get the net input matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        IX : The augmented feature matrix [1, X]\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The net input matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        return IX.dot(self.theta)\n",
    "    \n",
    "    def activation(self, net_input):\n",
    "        \"\"\"\n",
    "        Get the probability (sigmoid or softmax) matrix\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net_input : The net input\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            return (net_input >= 0) * 1          \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            # Get the exponent of the net input\n",
    "            net_input_exp = np.exp(net_input - np.max(net_input, axis=1).reshape(-1, 1))\n",
    "\n",
    "            # Return the softmax matrix\n",
    "            return net_input_exp / np.sum(net_input_exp, axis=1).reshape(-1, 1)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        The predict probability function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The probability (sigmoid or softmax) matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the augmented feature matrix [1, X]\n",
    "        IX = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "            \n",
    "        # Get the net_input matrix\n",
    "        N = self.net_input(IX)\n",
    "\n",
    "        return self.activation(N)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict class function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted class vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # If binary classification\n",
    "        if self.n_classes == 2:\n",
    "            return (self.predict_proba(X) >= 0.5) * 1         \n",
    "        # If multi-class classification\n",
    "        else:\n",
    "            return np.argmax(self.predict_proba(X), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqph1EErBRl3"
   },
   "source": [
    "### Multiple-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLrJ4jJJBRl3"
   },
   "source": [
    "The code below shows how to implement multiple-layer perceptron using mini-batch gradient descent and regularization (lasso, ridge and elastic net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 5444,
     "status": "error",
     "timestamp": 1602460487146,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "TLTUckKI11MR",
    "outputId": "2ebba743-1f48-4a14-e961-a42c23d92faa"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import numpy as np\n",
    "\n",
    "class MultiLayerPerceptron_MBGD(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Multi-layer perceptron implemented using mini-batch gradient descent and regularization (lasso, ridge and elastic net)\"\"\"\n",
    "        \n",
    "    def __init__(self, \n",
    "                 hidden_layer_sizes=[100], \n",
    "                 activation='relu', \n",
    "                 batch_size=32,\n",
    "                 learning_rate_init=0.01, \n",
    "                 max_iter=100, \n",
    "                 shuffle=True, \n",
    "                 penalty='l2',\n",
    "                 alpha=1, \n",
    "                 gamma=0.5,\n",
    "                 random_state=42):\n",
    "        \n",
    "        # The number of perceptrons on each hidden layer\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        \n",
    "        # The Activation function\n",
    "        self.activation = activation\n",
    "        \n",
    "        # The size of minibatches for stochastic optimizers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # The initial learning rate\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "\n",
    "        # The maximum number of epochs\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # Whether to shuffle samples in each epoch\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # The regularization term\n",
    "        self.penalty=penalty\n",
    "        \n",
    "        # The regularization parameter\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # The elastic net mixing parameter\n",
    "        self.gamma=gamma\n",
    "        \n",
    "        # The random state\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        The fit function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        y : the target vector\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the one-hot-encoded target matrix\n",
    "        Y = pd.get_dummies(y).values\n",
    "        \n",
    "        # Initialize the object variables\n",
    "        self.fit_init(X, Y)\n",
    "                \n",
    "        # For each epoch\n",
    "        for epoch in range(self.max_iter):\n",
    "            # Get the indices of the training data\n",
    "            idxs = np.array(range(X.shape[0]))\n",
    "            \n",
    "            # Get the minibatches of the training data\n",
    "            mbs = self.get_mbes(idxs)\n",
    "            \n",
    "            # For each minibatch\n",
    "            for mb in mbs:   \n",
    "                # Get the feature matrix\n",
    "                X_mb = X[mb,:]\n",
    "                                \n",
    "                # Get the target matrix\n",
    "                Y_mb = Y[mb,:]\n",
    "                                \n",
    "                # Update the weights and biases using mini-batch gradient descent\n",
    "                self.mini_batch_gradient_descent(X_mb.reshape(self.batch_size, -1), Y_mb.reshape(self.batch_size, -1))\n",
    "                \n",
    "                # Update the cost\n",
    "                self.costs[epoch] += np.sum((Y_mb.reshape(self.batch_size, -1) - self.activations[-1]) ** 2) / self.m\n",
    "                \n",
    "    def fit_init(self, X, Y):\n",
    "        \"\"\"\n",
    "        Initialize the object variables\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        y : the target matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the number of samples and featurs\n",
    "        self.m, self.n = X.shape\n",
    "        \n",
    "        # Initialize the number of unique class labels\n",
    "        self.classes = np.unique(Y)\n",
    "        \n",
    "        # Initialize the number of perceptrons on each layer\n",
    "        self.layer_sizes = [self.n] + self.hidden_layer_sizes + [len(self.classes)]\n",
    "        \n",
    "        # Initialize the cost\n",
    "        self.costs = np.zeros(self.max_iter)\n",
    "        \n",
    "        # Initialize the random number generator\n",
    "        self.rgen = np.random.RandomState(seed=self.random_state)\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.W = [0] + [self.rgen.normal(loc=0.0, \n",
    "                                         scale=2 / np.sqrt( self.layer_sizes[i - 1] + self.layer_sizes[i]), \n",
    "                                         size=(self.layer_sizes[i - 1], self.layer_sizes[i]))\n",
    "                        for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        # Initialize the biases\n",
    "        self.b = [0] + [np.zeros((1, self.layer_sizes[i]))\n",
    "                        for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        # Initialize the net inputs\n",
    "        self.net_inputs = [0] * (len(self.layer_sizes))\n",
    "\n",
    "        # Initialize the activations\n",
    "        self.activations = [0] * (len(self.layer_sizes))\n",
    "        \n",
    "        # Initialize the sensitivities\n",
    "        self.sensitivities = [0] * (len(self.layer_sizes))\n",
    "        \n",
    "        # Initialize the derivatives\n",
    "        self.derivatives = [0] * (len(self.layer_sizes))\n",
    "        \n",
    "    def get_mbes(self, idxs):\n",
    "        \"\"\"\n",
    "        Get the minibatches\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        idxs : The indices of the data\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The minibatches\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the minibatches\n",
    "        mbs = []\n",
    "        \n",
    "        if self.shuffle is True:\n",
    "            # Shuffle the indices\n",
    "            self.rgen.shuffle(idxs)\n",
    "                \n",
    "        # Get the number of minibatches\n",
    "        n_batch = len(idxs) // self.batch_size\n",
    "        \n",
    "        # For each minibatch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the minibatch\n",
    "            first_idx = i * self.batch_size\n",
    "            last_idx = min((i + 1) * self.batch_size, len(idxs))\n",
    "                                    \n",
    "            # Get the minibatch\n",
    "            mb = idxs[first_idx : last_idx]\n",
    "            \n",
    "            # Update the minibatches\n",
    "            mbs.append(mb)\n",
    "\n",
    "        return mbs\n",
    "                \n",
    "    def mini_batch_gradient_descent(self, X, Y):\n",
    "        \"\"\"\n",
    "        Update the weights and biases using mini-batch gradient descent\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        Y : the target matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the activation in the first layer\n",
    "        self.activations[0] = X\n",
    "\n",
    "        # Propagate the net input and activation forward through the network\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            # Get the net input on layer i\n",
    "            self.net_inputs[i] = self.get_net_input(i)  \n",
    "            \n",
    "            # Get the activation on layer i\n",
    "            self.activations[i] = self.get_activation(i)\n",
    "                                \n",
    "        # Get the sensitivity in the last layer\n",
    "        self.sensitivities[-1] = - 2 / X.shape[0] * np.array([np.matmul((Y - self.activations[-1])[k, :],\n",
    "                                                                        self.get_derivative(len(self.layer_sizes) - 1)[k, :, :]) \n",
    "                                                              for k in range(X.shape[0])])\n",
    "                        \n",
    "        # Propagate the sensitivites backward through the network\n",
    "        for i in range(len(self.layer_sizes) - 2, 0, -1):\n",
    "            # Get the derivative on layer i\n",
    "            self.derivatives[i] = self.get_derivative(i)\n",
    "                        \n",
    "            # Get the sensitivity on layer i\n",
    "            self.sensitivities[i] = self.get_sensitivity(i)\n",
    "                        \n",
    "        # Update the weights and biases using gradient descent\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            # Update the biases\n",
    "            self.b[i] -= self.learning_rate_init * self.sensitivities[i].sum(axis=0).reshape(1, -1)\n",
    "            \n",
    "            # Update the weights\n",
    "            self.W[i] -= self.learning_rate_init * (np.matmul(self.activations[i - 1].T, self.sensitivities[i]))\n",
    "                                                    \n",
    "            # If lasso\n",
    "            if self.penalty == 'l1':\n",
    "                self.W[i] -= self.learning_rate_init * self.alpha * np.sign(self.W[i])                        \n",
    "            # If ridge\n",
    "            elif self.penalty == 'l2':\n",
    "                self.W[i] -= self.learning_rate_init * self.alpha * self.W[i]                                              \n",
    "            # If elastic net\n",
    "            elif self.penalty == 'elasticnet':\n",
    "                self.W[i] -= self.learning_rate_init * self.alpha * (self.gamma * np.sign(self.W[i]) + (1 - self.gamma) * self.W[i])                                              \n",
    "            \n",
    "    def get_net_input(self, i):\n",
    "        \"\"\"\n",
    "        Get the net input on layer i\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : the ith layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The net input on layer i\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.b[i] + np.matmul(self.activations[i - 1], self.W[i]) \n",
    "    \n",
    "    def get_activation(self, i):\n",
    "        \"\"\"\n",
    "        Get the activation on layer i\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : the ith layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The activation on layer i    \n",
    "        \"\"\" \n",
    "\n",
    "        if self.activation == 'identity':\n",
    "            return self.net_inputs[i]\n",
    "        elif self.activation == 'logistic':\n",
    "            return 1 / (1 + np.exp(-self.net_inputs[i]))\n",
    "        elif self.activation == 'tanh':\n",
    "            e_z = np.exp(self.net_inputs[i])\n",
    "            e_neg_z = np.exp(-self.net_inputs[i])\n",
    "            return (e_z - e_neg_z) / (e_z + e_neg_z)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.clip(self.net_inputs[i], 0, None)\n",
    "        else:\n",
    "            print(\"Activation undefined!\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def get_derivative(self, i):\n",
    "        \"\"\"\n",
    "        Get the derivative on layer i\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : the ith layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The derivative on layer i    \n",
    "        \"\"\" \n",
    "        \n",
    "        if self.activation == 'identity':\n",
    "            return np.array([np.identity(self.activations[i].shape[1]) \n",
    "                             for j in range(self.activations[i].shape[0])])\n",
    "        elif self.activation == 'logistic':        \n",
    "            return np.array([np.diag((self.activations[i] * 1 - self.activations[i])[j, :]) \n",
    "                             for j in range(self.activations[i].shape[0])])\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.array([np.diag((1 - self.activations[i] ** 2)[j, :]) \n",
    "                             for j in range(self.activations[i].shape[0])])\n",
    "        elif self.activation == 'relu':         \n",
    "            return np.array([np.diag(np.where(self.net_inputs[i][j, :] >= 0, 1, 0).reshape(-1)) \n",
    "                             for j in range(self.net_inputs[i].shape[0])])\n",
    "        else:\n",
    "            print(\"Activation undefined!\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "    def get_sensitivity(self, i):\n",
    "        \"\"\"\n",
    "        Get the sensitivity on layer i\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        i : the ith layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The sensitivity on layer i    \n",
    "        \"\"\" \n",
    "        \n",
    "        # Get matrix multiplication\n",
    "        M = np.matmul(self.sensitivities[i + 1], self.W[i + 1].T)\n",
    "        \n",
    "        return np.array([np.matmul(M[k, :], self.derivatives[i][k, :, :]) \n",
    "                         for k in range(M.shape[0])])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        The predict function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : the feature matrix\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        The predicted class labels of the target\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the net inputs\n",
    "        self.net_inputs = [0] * (len(self.layer_sizes))\n",
    "        \n",
    "        # Initialize the activations\n",
    "        self.activations = [0] * (len(self.layer_sizes))\n",
    "        # Initialize the activation in the first layer\n",
    "        self.activations[0] = X\n",
    "\n",
    "        # Propagate the input forward through the network\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            # Get the net input on layer i\n",
    "            self.net_inputs[i] = self.get_net_input(i)  \n",
    "            \n",
    "            # Get the activation on layer i\n",
    "            self.activations[i] = self.get_activation(i)\n",
    "                                \n",
    "        return np.argmax(self.activations[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1601408798539,
     "user": {
      "displayName": "Huang Yuxiao",
      "photoUrl": "",
      "userId": "05167076769245149404"
     },
     "user_tz": 240
    },
    "id": "gnRPDPDGBRl5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pmlm_models_shallow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
